\section{Detecting False Sharing}
\label{sec:detect}

% What is the basic idea? Why those ones uses performance counters can not reveal false sharing problems? 
\cheetah{} only reports false sharing problems with significant performance impact, where they cause a large number of cache invalidations for corresponding cache lines. However, this turns out to be difficult because cache invalidations depend on memory access patterns, cache hierarchies, and thread-to-core mappings. 
To address this challenge, \cheetah{} proposes a simple rule to track cache invalidations: \emph{when a thread writes a cache line after other threads have accessed the same cache line, this write access causes a cache invalidation}. This rule is based on the following two assumptions:
 
%\todo{Adding a overview figure about the system.}
%Though hardware performance counters can collect events related to the cache invalidation, they have difficulties in identifying its root causes, such as instructions and data objects involved in the cache invalidation. In contrast, directly analyzing the memory traces collected by PMU can give more insights. Given a set of memory addresses, one typically needs the configuration of cache hierarchy and the knowledge of thread-to-core mappings to perform accurate analysis. 

\begin{itemize} 
\item {\bf Assumption 1:} Each thread runs on a separate core with its own private cache. 

\item {\bf Assumption 2: } Cache sizes are infinite. 
 
\end{itemize}

Assumption 1 is reasonable because thread over-subscription is generally rare for computation intensive programs. This assumption may over-estimate the number of cache invalidations, if multiple threads are actually scheduled to the same physical core or different cores sharing part of cache hierarchy. Under this circumstance, Assumption 1 shows the most number of cache invalidations for a given false sharing instance. With this assumption, \cheetah{} does not have to track thread-to-core mapping or know the actual cache hierarchy. 

Assumption 2 further defines the behavior related to cache eviction and invalidation. 
%According to assumption 2, since the size of a cache is infinite, data will be ways kept in the cache.   
If there is a memory access within a cache line, the hardware cache (of running this thread) always holds the data until an access issued by other threads (running on other cores by assumption 1) invalidates it since the size of the cache is infinite. This assumption avoids tracking cache evictions caused by the cache capacity. 
%However, 
%If there is a memory access within a cache line, the hardware cache (of running this thread) always holds the data until an access of other threads (running on other cores by assumption 1) invalidates it. Thus, there is no need to track cache evictions that are caused by cache capacity. 
By combing these two assumptions, \cheetah{} identifies cache validations simply based on memory access patterns, independent from the architecture configurations and parallel execution environments~\cite{Predator, qinzhao}. 

In the remaining of this section, we elaborate how we track memory accesses in Section~\ref{sec:perfcounter}, how we locate a sampled access's cache line in Section~\ref{sec:shadow}, how we analyze access patterns for cache invalidations in Section~\ref{sec:computeinvalidations}, and how we report false sharing in Section~\ref{sec:report}.

\subsection{Sampling Memory Accesses}
\label{sec:perfcounter}

According to the basic rule described above, it is important to track memory accesses of different threads in order to compute the number of cache invalidations on each cache line. Software based approaches may introduce higher than $5\times$ performance overhead~\cite{Predator, qinzhao}. This high overhead can block people from using these tools in real deployment.

%The state-of-the-art work \Predator{} leverages the compiler instrumentation to insert function calls before every memory access~\cite{Predator}. However, this approach introduces more than $5\times$ performance overhead by instrumenting every memory access. Moreover, \Predator{} has to re-compile applications, which needs the availability of source code. \Predator{} is not desirable for legacy applications without the source code, or real deployment that is sensitive to performance. 
\cheetah{} significantly reduces the performance overhead by leveraging PMU-based sampling mechanisms, such as AMD instruction-based sampling (IBS)~\cite{AMDIBS:07} and Intel precise event-based sampling (PEBS)~\cite{IntelArch:PEBS:Sept09}, pervasively available in modern CPU architectures. For each sample, PMU distinguishes whether it is a memory read or write, captures the memory address touched by the sampled memory access, and records the thread ID that triggers this sample. Those raw data will be utilized to analyze whether the sampled access incurs a cache invalidation or not, based on the basic rule described in Section~\ref{sec:detect}.

%These information is enough to compute the number of cache invalidations based on the basic rule that is described in Section~\ref{sec:basicidea}. 
Since PMU only samples one memory access out of a specified number, typically with a sampling frequency of tens of samples per second per thread, it doesn't pose significant performance overhead. Besides that, using PMU-based sampling does not need to instrument source or binary code explicitly, thus providing a non-intrusive way to monitor memory references. \cheetah{} shows that the PMU-based sampling, even with sparse samples (one out of 64K), can identify false sharing with significant performance impact.
%Besides, the rich events recorded by PMU, such as cache misses and latency, can be used for further analysis. 
%However, there are still challenges for using PMU-based sampling, which does monitor all memory accesses in the whole program or a execution window. 
 

\paragraph{Implementation.} 

In order to sample memory accesses, \cheetah{} programs the PMU registers to turn on sampling with a frequency of one out of 64K instructions, before entering the \texttt{main} routine of the application. It also installs a signal handler to associate with the sampling so that we can collect detailed memory accesses. In order to simplify the signal handling, \Cheetah{} configures the signal handler to be responded by the current thread, by calling \texttt{fcntl} function with \texttt{F\_SETOWN\_EX} flag. Inside the signal handler, \Cheetah{} collects detailed information of every sampled memory access, including its memory address, thread id, read or write operation, and access latency, which can be fed into the ``FS detection'' one out of 64K to compute the number of cache invalidations and the ``FS assessment'' module to predict the performance impact.

\subsection{Locating Cache Line}
\label{sec:shadow}

For each sampled memory access, \cheetah{} will decide whether this access causes a cache invalidation or not and records the detailed information about this access. For this purpose, \cheetah{} should quickly locate an access's related cache line. \Cheetah{} also utilizes the shadow memory mechanism to speed up this locating procedure~\cite{qinzhao, Predator}. 
%Shadow memory has been utilized extensively in different fields, such as detecting concurrency bugs~\cite{Harrow:2000:RCM:645880.672080, helgrind, 404681, Savage:1997:EDD:268998.266641}, tracking information flow or data flow~\cite{Cheng:2006:TEF:1157733.1157903, Newsome05dynamictaint, Qin:2006:LLP:1194816.1194834}, or detecting memory errors or others~\cite{qinzhao, Hastings91purify:fast, Seward:2005:UVD:1247360.1247362, Narayanasamy:2006:ALO:1140277.1140303}.  
% FIXME, if we need less citations. 
To utilize the shadow memory mechanism, we should determine the range of heap memory, which is difficult to know beforehand if using the default heap. Thus, \cheetah{} builds its custom heap based on Heap Layers~\cite{Berger:2001:CHM:378795.378821}. \cheetah{} pre-allocates a fixed size of memory block (using \texttt{mmap}) and satisfies all memory allocations from that. \cheetah{} adapts the per-thread heap organization used by Hoard so that two objects in the same cache line will never be allocated to two different threads~\cite{Hoard}. This design prevents inter-objects false sharing, but also makes \cheetah{}  cannot report problems possibly caused by the default heap allocator.  
%However, we argue that this problem should be fixed by using a modern heap allocator like Hoard~\cite{Hoard}. 

\paragraph{Implementation} 
To use its custom heap, \cheetah{} intercepts all memory allocations and deallocations. \cheetah{} initializes the heap before an application enters \texttt{main} routine, by putting the initialization routine into the constructor attribute. \cheetah{} maintains two different heaps, one for the application and one for internal uses. For both heaps, \cheetah{} manages objects based on the unit of {\it power of two}. For each memory allocation from the application, \cheetah{} saves the information of callsite and size, which helps \cheetah{} to precisely report the line information of falsely-shared objects.  
%heap and globals. 
%Two huge arrays. 
%\cheetah{} keeps track of memory accesses of global variables and objects of the application heap using the shadow memory technique.
\Cheetah{} allocates two large arrays (using \texttt{mmap}) to keep track of the number of writes and detailed access information on each cache line. For each memory access, \cheetah{} uses the bit shift to compute the index of cache line so that it can quickly locate its corresponding cache line for every sampled memory access. 


\subsection{Computing Cache Invalidations}
\label{sec:computeinvalidations}

%Section~\ref{sec:detect} discusses a general rule to compute the number of cache invalidations. 
Prior work of Zhao et. al. proposed a method based on the ownership of cache lines to compute the number of cache invalidations: when a thread updates an object owned by others, this access causes an cache invalidation and resets the owner to the current thread~\cite{qinzhao}. But this approach cannot easily scale to more than 32 threads because of excessive memory consumption, since it needs a bit for every thread to track the ownership.  

%To track the number of cache invalidations,  \Cheetah{} keeps a counter for every cache line.  
To address this problem, \Cheetah{} maintains a two-entry table ($T$) for each cache line ($L$), with eight words in total. In this table, each entry has two fields, thread ID and access type (read or write). It computes the invalidations according to the rule described in Section~\ref{sec:detect}. In case of a cache invalidation, the current access (write) will be added into the corresponding table. Thus, each table always has at least an entry. More specifically, \cheetah{} checks possible cache invalidations as follows.

%a write access from a thread different with existing entries can cause a cache invalidation. When there is a cache invalidation, 
\begin{itemize}
\item
For each read access, \cheetah{} will decide whether to record this entry. If the table $T$ is not full and the existing entry is coming from a different thread (with a different ID), \cheetah{} will record this read access in the table.
  
  \item
  For each write access, \cheetah{} will decide whether this access incurs an invalidation. If the table is already full, it will lead to a cache invalidation since at least an existing entry is from a different thread (assumption 1). If the table is not full (and not empty), a write access will incur a cache invalidation only if the existing entry is from a different thread. \cheetah{} does nothing if this write access is from the same thread as the existing entry. 
  
\end{itemize}
     
%When there is an memory access, \Cheetah{} checks against its two-entries-cache-history table for the current cache line and determines whether an access leads to a cache invalidation according to the rule discussed above. 

\paragraph{Implementation} 
As aforementioned, only cache lines with a big number of writes can possibly have a big impact on the performance. Based on this observation, cache lines with a small number of writes are never be a target that can cause serious performance degradation. For this reason, \Cheetah{} tracks the number of writes on a cache line at first, and only tracks detailed information when this number is larger than a pre-defined threshold. Using this method can also save memory, since \cheetah{} only allocates memory to record word-level read/write access: what is the address of an access; whether this is a read or write access; which thread issues this access. All of these are going to be checked in the reporting phase, described in the next section. 

 \subsection{Reporting False Sharing}
% How we will report false sharing precisely and correctly?
% How we 
\label{sec:report}

\Cheetah{} reports false sharing correctly and precisely, either at the end of programs or receiving instructions from users.  

\paragraph{Correct Detection.} \Cheetah{} tracks word-based (four bytes) memory accesses on susceptible cache lines: how many reads or writes issued by a specific thread on each word. When more than one thread access a word, \Cheetah{} marks this word to be \emph{shared}. By identifying accesses on each word on a susceptible cache line, we can easily differentiate false sharing from true sharing. Word-based information can also help diagnose false sharing problems in more detail, which helps programmers to decide how to pad a problematic data structure in order to avoid false sharing. Because it is possible for a thread, particularly the main thread, to allocate an object and initialize it before being accessed by other threads, \cheetah{} only tracks the behavior of memory accesses inside parallel regions.
% to avoid this problem.

\paragraph{Precise Detection.} \Cheetah{} reports precise information for global variables and heap objects that are involved in false sharing. For global variables, \Cheetah{} reports names and addresses by searching through the ELF symbol table. For heap objects, \Cheetah{} reports the lines of code for their allocation sites. Thus, \Cheetah{} intercepts all memory allocations and de-allocations to obtain the whole call stack. \cheetah{} does not monitor stack variables because they are normally accessed only by their hosting threads. 
%\todo{Check the last sentence I added is correct or not.} 
%During the real implementation, we tried to keep the overhead of getting the callsite stack as little as possible. \cheetah{} utilizes a global hash table to save those known callsite stack. The combination of ``rip'' (instruction pointer) and ``stack offset'' is considered as the key of this global hash table. If the combination of these two values (as the key) have existed in the global hash table, we simply copied the saved callsite stack to a new object. Otherwise, backtrace() is called to fetch the callstack.  

 
%\cheetah{} only reports those false sharing problems that can have a significant impact on the performance by predicting the upper bound of performance improvement, according to the idea that are discussed in Section~\ref{sec:predictidea}.  It will rank the severity of performance degradation of any detected false sharing problems based on the predicted performance improvement after fixes.