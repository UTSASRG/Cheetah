\section{Detecting False Sharing}
\label{sec:detect}

% What is the basic idea? Why those ones uses performance counters can not reveal false sharing problems? 
\cheetah{} aims to report false sharing that has a significant performance impact to the whole program execution, that is, a large number of cache invalidations occur in cache lines. Though hardware performance counters can collect events related to the cache invalidation, they have difficulties in identifying its root causes, such as instructions and data objects involved in the cache invalidation. In contrast, directly analyzing the memory traces collected by PMU can give more insights. Given a set of memory addresses, one typically needs the configuration of cache hierarchy and the knowledge of thread-to-core mappings to perform accurate analysis. 
\cheetah{}, designed to be independent from architectures and runtime environments, detect cache invalidations based on a rule: \emph{If a thread writes a cache line after other threads have accessed the same cache line, this write operation causes a cache invalidation}. To match the rule with the practice, \cheetah{} further makes the following two assumptions:
\begin{itemize} 
\item {\bf Assumption 1:} Each thread runs on a separate core with its own private cache. 

\item {\bf Assumption 2: } Cache sizes are infinite. 
 
\end{itemize}

Assumption 1 is reasonable because thread over-subscription is rare for computation intensive programs. Moreover, given different architectures and operating systems, threads may be scheduled to different cores. Thus, even in one execution, threads are scheduled to the same core, we still need to conservatively assume that they can run on different cores during different executions.
%If multiple threads are actually running on the same core, false sharing problems may cause less performance penalty. 
To this end, Assumption 1 actually presents the worst scenario for a given false sharing. 
With this assumption, \cheetah{} is independent from thread-to-core mapping.
%does not need to know the actual situation where a thread is running by assuming different threads are running on different cores; there is no need to know actual cache hierarchy by assuming different cores having their own cache (at least L1). 

Assumption 2 conservatively assumes that every access can be a hit to the cache. Only both reads and writes to the data in the cache, cache invalidation may occur. Otherwise, if the data is in main memory, accessing it does not incur any cache invalidation. This assumption prevents \cheetah{} from depending on the cache capacity and also assumes the worst scenario, like Assumption 1.
%However, 
%If there is a memory access within a cache line, the hardware cache (of running this thread) always holds the data until an access of other threads (running on other cores by assumption 1) invalidates it. Thus, there is no need to track cache evictions that are caused by cache capacity. 

With applying both of these assumptions, \cheetah{} can identify cache validations by simply checking memory accesses patterns without knowing the actual memory hierarchy and parallel execution environments: if a memory access of a thread will load a cache line into its own private cache (assumption 1), only a write from a different thread can invalidate this cache line. Thus, we can report the number of cache validations by analyzing memory traces only, similar to the previous work~\cite{Predator, qinzhao}. 

{\color{red}\Cheetah{} is a library that can be preloaded (using \texttt{LD\_PRELOAD} or can be linked to: there is no need to change or recompile the programs, or to modify the underlying operating system. } {\color{blue} Do you want to put this sentence here?}
%This idea is very similar to Predator~\cite{Predator}. \Cheetah{} will rely on hardware-performance counters to reduce performance overhead of detection, instead of a instrumentation-based approach~\cite{Predator}. 

 
% This idea is similar to Predator. But Predator is a compiler-based approach, which needs to change the source code of applications. Also, it introduces much performance overhead that can block its use in deployment environment. 

% To reduce the performance overhead, 
% Basic idea, by examing the memory access pattern
\subsection{Shadow Memory and Custom Heap}

For each sampled memory access, \cheetah{} has to locate its specific cache line so that we can compute the number of cache invalidations. \Cheetah{} utilizes the shadow memory mechanism to speedup the locating process. Shadow memory has been utilized extensively in different fields, such as detecting concurrency bugs~\cite{Harrow:2000:RCM:645880.672080, helgrind, 404681, Savage:1997:EDD:268998.266641}, tracking information flow or data flow~\cite{Cheng:2006:TEF:1157733.1157903, Newsome05dynamictaint, Qin:2006:LLP:1194816.1194834}, or detecting memory errors or others~\cite{qinzhao, Hastings91purify:fast, Seward:2005:UVD:1247360.1247362, Narayanasamy:2006:ALO:1140277.1140303}.  

To utilize the shadow memory mechanism, we should determine the range of heap memory, which is difficult to know beforehand if using the default heap. \cheetah{} built its custom heap based on Heap Layers~\cite{Berger:2001:CHM:378795.378821}. \cheetah{} pre-allocates a fixed size of memory
from its underlying operating system using \texttt{mmap} system calls and satisfies memory allocations from this block of memory. \cheetah{} also adapts the per-thread heap organization used by Hoard~\cite{Hoard} so that two objects in the same cache line will never be allocated to two different threads. This design prevents inter-objects false sharing, but also makes \cheetah{} can not report false sharing problems caused by the heap allocator.  However, we argue that this problem should be fixed by using a modern heap allocator like Hoard~\cite{Hoard}. 

\paragraph{Implementation} 
To use its custom heap, \cheetah{} intercepts all memory allocations and deallocations. \cheetah{} initializes the heap before the application enters into \texttt{main} routine, by putting the initialization routine into the constructor attribute. \cheetah{} maintains two different heaps, where all memory usage of the application will be satisfied from its application heap and other memory usage will be allocated from its  internal heap. For both heaps, \cheetah{} manages objects based on the unit of {\it power of $2$}. For each memory allocation from the application, \cheetah{} saves the information of callsite and size by adding an object header to each object. This helps \cheetah{} to precisely report the line information of falsely-shared objects.  

%heap and globals. 
%Two huge arrays. 
\cheetah{} keeps track of memory accesses of global variables and objects of the application heap using the shadow memory technique. \Cheetah{} allocates two large arrays (by using \texttt{mmap}) to keeping track of the number of writes and detailed access information on each cache line. For each memory address, \cheetah{} uses the bit shift to compute the index of cache line and locates the placement of its corresponding array quickly. 

\subsection{Sampling Memory Accesses}
\label{sec:perfcounter}

According to the basic rule described above, it is very important to track memory accesses of different threads in order to compute the number of cache invalidations on each cache line. 

The previous work \Predator{} leverages the compiler instrumentation to insert function calls before every memory access~\cite{Predator}. However, this approach introduces more than $5\times$ performance overhead by instrumenting every memory access. More than that, \Predator{} has to re-compile applications, which needs the availability of source code. \Predator{} is not desirable for legacy applications without the source code, or real deployment that is sensitive to performance. 

\cheetah{} aims to significantly reduce the performance overhead by leveraging hardware performance counters that are available on most modern hardware. Hardware performance counters provide sampling-based memory access information about how the hardware is being exercised by a program, such as read/write operations, memory addresses, and threads~\cite{Mucci99papi}. These information is enough to compute the number of cache invalidations based on the basic rule that is described in Section~\ref{sec:basicidea}. Since hardware performance counter only samples a memory access out of a specified number, it doesn't pose significant performance overhead. 
Besides that, using performance counters does not need to instrument source code explicitly, thus providing a non-intrusive way to monitor memory references. 

\cheetah{} is different with existing approaches using hardware performance counters to detect false sharing problems~\cite{mldetect, openmp, detect:ptu}. Jayasena et. al. collects different types of events like memory accesses, data caches, TLBs, interactions among cores, and resources stalls, and derives potential memory patterns that can cause false sharing~\cite{mldetect}. DARWIN collects cache coherence events at the first round, then identifies possible memory accesses on those data structures that are involved in frequent cache invalidations for the second round~\cite{openmp}. DARWIN also requires manual effort or expertise to verify whether false sharing occurs or not.  Intel's PTU also relies on memory sampling mechanism but can not differentiate false sharing and true sharing since it discards the temporal information of memory accesses~\cite{detect:ptu}. More details on the difference have been discussed in Section~\ref{sec:relatedwork}.

\paragraph{Implementation} 
In order to sample memory accesses, \cheetah{} has to setup hardware registers before entering the \texttt{main} routine of the application.  \Cheetah{} installs a signal handler so that it can collect detailed information of memory accesses after a specified sample period. In order to simplify the signal handling, \Cheetah{} also configures the signal handler to be responded by the current thread, by calling \texttt{fcntl} function with \texttt{F\_SETOWN\_EX} flag. \cheetah{} also sets up the custom allocator and its internal heap in the initialization.

Inside the signal handler, \Cheetah{} collects detailed information of every sampled memory access, including its memory address, thread id, read or write operation, and access latency, which can be fed into the computing module to compute the number of cache invalidations and the prediction module to predict performance impact.


\subsection{Computing Cache Invalidations}
\label{sec:computeinvalidations}

\Cheetah{} targets to report false sharing that can have performance impact on applications. \Cheetah{} focuses on those false sharing problems with a significant number of cache invalidations.  

Qin Zhao et. al. propose to compute the number of cache invalidations based on the ownership of cache lines: when a thread updates an object that it does not own, it will cause cache invalidations and set the owner to the current thread~\cite{qinzhao}. However, this approach cannot be easily to scalable to more than 32 threads because of excessive memory consumption, where every word with 32-bits can only tracking 32 threads. Also it introduce significant performance overhead by utilizing the dynamic instrumentation technique and maintaining ownership of different cache lines. 

To overcome these shortcomings, \Cheetah{} maintains a two-entries-table ($T$) for each cache line ($L$) that is borrowed from Predator~\cite{Predator}, where each entry tracks accesses from one thread in a period. \Cheetah{} also keeps a counter for every cache line that indicates the number of cache invalidations on this cache line.  
According to the basic rule that are described in Section~\ref{sec:basicidea}, only a write access can cause a cache invalidation. When there is a cache invalidation, the current write access will flush the table and will be added into its corresponding table ($T$). Thus, a table will always have an entry except in the beginning. More specifically, \cheetah{} checks possible cache invalidations as follows.
 
\begin{itemize}
\item
  For each read access $R$, \cheetah{} will check: 
  \begin{itemize}
    \item
      If $T$ is full, there is no need to record this read access.
    \item
      If $T$ is not full and the existing entry has a different thread ID, 
      then \cheetah{} records this read access by adding a new entry to the table.
  \end{itemize}
\item
  For each write access $W$,  
  \begin{itemize}
    \item
      If $T$ is full, then $W$ causes a cache invalidation since at least one of two existing entries are issued by another thread (on another core).
    \item
      If $T$ is not full (and not empty),
      \cheetah{} checks whether the current $W$ is from the same thread as the existing entry . If
      so, $W$ will not cause a cache invalidation. Otherwise, there is a cache invalidation caused by this $W$.
  \end{itemize}
\end{itemize}

      
In the end, \cheetah{} can collect the number of cache invalidations happened on each cache line. 

\paragraph{Implementation} 

When there is an memory access, \Cheetah{} checks against its two-entries-cache-history table for the current cache line and determines whether an access leads to a cache invalidation according to the rule discussed above. 

Since \cheetah{} only cares about those cache lines that can potentially involve in false sharing. We observe that only those cache lines with a big number of writes can possibly cause a lot of cache invalidations. Based on this observation, cache lines with a small number of writes are never be a target that can cause serious performance problem. For this reason, \Cheetah{} tracks  the number of writes on a cache line at first, and only tracks detailed information when this number is larger than a pre-defined threshold, which we refer to as the {\it Tracking-Threshold}. To save memory, \cheetah{} only allocates memory to record the detailed information for all words in this cache line after this threshold.
 
After this threshold is reached, \Cheetah{} will track detailed read/write information for each access: what the address of an access; whether this is a read or write access; which thread issues this access. These information are going to be checked in the reporting phase that is described in Section~\ref{sec:report}. 

 \subsection{Reporting False Sharing}
% How we will report false sharing precisely and correctly?
% How we 
\label{sec:report}

\Cheetah{} aims to report false sharing correctly and precisely, same as existing work~\cite{Sheriff, Predator}. \Cheetah{} will invoke the process of checking and reporting false sharing problems, either at the end of programs or receiving the instructions from users through the \texttt{SIGUSR2} signal.  

\paragraph{Correct Detection} \Cheetah{} keeps track of word-based (four bytes) memory accesses on susceptible cache lines: how many reads or writes occurs by which thread on each word. When more than one thread access a word, \Cheetah{} marks this word to be shared. By identifying accesses on each word on a susceptible cache line, we can easily differentiate false sharing from true sharing, as shown in Figure~\ref{fig:falsesharing}. Word-based information can also help diagnose false sharing problems in more detail, which helps programmers to decide how to padding an existing data structure in order to avoid false sharing. Because it is possible for a thread, particularly the main thread, to allocate an object and do some initialization before passing to different threads,  \cheetah{} only tracks the information of memory accesses inside parallel phases to avoid this problem.

\paragraph{Precise Detection} \Cheetah{} reports precise information for global variables and heap objects that are involved in false sharing. For global variables, \Cheetah{} reports names and addresses by searching through the ELF symbol table. For heap objects, \Cheetah{} reports the lines of code for allocating these objects.  
Thus, \Cheetah{} intercepts all memory allocations and de-allocations and utilizes \texttt{backtrace()} to obtain the whole callsite stack. 
%During the real implementation, we tried to keep the overhead of getting the callsite stack as little as possible. \cheetah{} utilizes a global hash table to save those known callsite stack. The combination of ``rip'' (instruction pointer) and ``stack offset'' is considered as the key of this global hash table. If the combination of these two values (as the key) have existed in the global hash table, we simply copied the saved callsite stack to a new object. Otherwise, backtrace() is called to fetch the callstack.  

 
%\cheetah{} only reports those false sharing problems that can have a significant impact on the performance by predicting the upper bound of performance improvement, according to the idea that are discussed in Section~\ref{sec:predictidea}.  It will rank the severity of performance degradation of any detected false sharing problems based on the predicted performance improvement after fixes.