\section{Detecting False Sharing}
\label{sec:detect}

% What is the basic idea? Why those ones uses performance counters can not reveal false sharing problems? 
\cheetah{} reports false sharing problems with a significant performance impact, where they will incur a large number of cache invalidations on corresponding cache lines. However, tracking cache invalidations turns out to be difficult because invalidations depend on the memory access pattern, cache hierarchy, and thread-to-core mappings. 
To address this challenge, \cheetah{} proposes a simple rule to compute cache invalidations: \emph{when a thread writes a cache line that has been accessed by other threads recently, this write access incurs a cache invalidation}. This rule is based on the following two assumptions, which are introduced in prior work~\cite{Predator, qinzhao}. 
 
%\todo{Adding a overview figure about the system.}
%Though hardware performance counters can collect events related to the cache invalidation, they have difficulties in identifying its root causes, such as instructions and data objects involved in the cache invalidation. In contrast, directly analyzing the memory traces collected by PMU can give more insights. Given a set of memory addresses, one typically needs the configuration of cache hierarchy and the knowledge of thread-to-core mappings to perform accurate analysis. 

\begin{itemize} 
\item {\bf Assumption 1:} Each thread runs on a separate core with its own private cache. 

\item {\bf Assumption 2: } Cache sizes are infinite. 
 
\end{itemize}

Assumption 1 is reasonable because the over-subscription of threads is generally rare for computation intensive programs. But we may {\bf overreport} the number of cache invalidations in the following situations: (1) if multiple threads are actually scheduled to the same physical core, or (2) different cores may share part of the cache hierarchy (instead of having private caches), or (3) the hyper-threading technology is enabled so that multiple threads may share the same CPU. However, we argue that the overreport problem can actually cancel out the weakness of using the sampling technique. 
%At worst, assumption 1 presents the largest number of cache invalidations for a given false sharing instance.
This assumption avoids the tracking of thread-to-core mapping or the knowing of the actual cache hierarchy.

Assumption 2 further defines the behavior related to cache eviction. In reality, a cache entry will be evicted when the cache is not sufficient to hold all active data of an application. Without this assumption, we should track every memory access and simulate the cache eviction in order to determine the accurate number of cache invalidations, which is notoriously slow but with a limited precision~\cite{falseshare:simulator}. Assumption 2 may overestimate the number of cache invalidations as the first assumption. Based on this assumption,  
if there is a memory access within a cache line, the hardware cache (of running this thread) always holds the data until an access issued by other threads (running on other cores by assumption 1) invalidates it. %If there is a memory access within a cache line, the hardware cache (of running this thread) always holds the data until an access of other threads (running on other cores by assumption 1) invalidates it. Thus, there is no need to track cache evictions that are caused by cache capacity. 
By combing these two assumptions, \cheetah{} identifies cache validations simply based on memory access patterns, independent from the underlying architecture and specific execution conditions. 

In the remaining of this section, we elaborate how we track memory accesses in Section~\ref{sec:perfcounter}, how we locate a sampled access's cache line in Section~\ref{sec:shadow}, how we compute cache invalidation based on sampled memory accesses in Section~\ref{sec:computeinvalidations}, and how we report false sharing in Section~\ref{sec:report}.

\subsection{Sampling Memory Accesses}
\label{sec:perfcounter}

According to the basic rule described above, it is important to track memory accesses in order to compute the number of cache invalidations on each cache line. Software-based approaches may introduce higher than $5\times$ performance overhead~\cite{Predator, qinzhao}. High overhead can block people from using these tools in real deployment.

%The state-of-the-art work \Predator{} leverages the compiler instrumentation to insert function calls before every memory access~\cite{Predator}. However, this approach introduces more than $5\times$ performance overhead by instrumenting every memory access. Moreover, \Predator{} has to re-compile applications, which needs the availability of source code. \Predator{} is not desirable for legacy applications without the source code, or real deployment that is sensitive to performance. 
\cheetah{} significantly reduces the performance overhead by leveraging PMU-based sampling mechanisms that are pervasively available in modern CPU architectures, such as AMD instruction-based sampling (IBS)~\cite{AMDIBS:07} and Intel precise event-based sampling (PEBS)~\cite{IntelArch:PEBS:Sept09}. For each sample, PMU distinguishes whether it is a memory read or write, captures the memory address, and records the thread ID that triggers this sample. These raw data will be analyzed whether the sampled access incurs a cache invalidation or not, based on the rules described in Section~\ref{sec:detect}.

%These information is enough to compute the number of cache invalidations based on the basic rule that is described in Section~\ref{sec:basicidea}. 
Since PMUs only sample one memory access out of a predefined number of accesses, this approach greatly reduces the runtime overhead incurred by performance data collection and analysis. Moreover, using PMU-based sampling does not need to instrument source or binary code explicitly, thus providing a non-intrusive way to monitor memory references. \cheetah{} shows that the PMU-based sampling, even with sparse samples (one out of 64K instructions), can identify false sharing with a significant performance impact.
%Besides, the rich events recorded by PMU, such as cache misses and latency, can be used for further analysis. 
%However, there are still challenges for using PMU-based sampling, which does monitor all memory accesses in the whole program or a execution window. 
 

\paragraph{Implementation.} 

<<<<<<< HEAD
In order to sample memory accesses, \cheetah{} programs the PMU registers to turn on sampling before the \texttt{main} routine. It also installs a signal handler to collect detailed memory accesses. \Cheetah{} configures the signal handler to be responded by the current thread, by calling \texttt{fcntl} function with \texttt{F\_SETOWN\_EX} flag. This method avoids the lookup overhead and  simplify the signal handling. Inside the signal handler, \Cheetah{} collects detailed information of every sampled memory access, including its memory address, thread ID, read or write operation, and access latency, which can be fed into the ``FS detection'' module to compute the number of cache invalidations and the ``FS assessment'' module to predict the performance impact.
=======
In order to sample memory accesses, \cheetah{} programs the PMU registers to turn on sampling before the \texttt{main} routine. It also installs a signal handler to collect detailed memory accesses. In order to simplify the signal handling, \Cheetah{} configures the signal handler to be responded by the current thread to balance the response of interrupt signals. It calls \texttt{fcntl} function with \texttt{F\_SETOWN\_EX} flag. Inside the signal handler, \Cheetah{} collects detailed information of every sampled memory access, including its memory address, thread ID, read or write operation, and access latency, which can be fed into the ``FS detection'' module to compute the number of cache invalidations and the ``FS assessment'' module to predict the performance impact.
>>>>>>> b5fe6c614fb15d46d82cb75a8d243a6b1f6b828b

\subsection{Locating Problematic Cache Lines}
\label{sec:shadow}

For each sampled memory access, \cheetah{} decides whether this access causes a cache invalidation or not, and records this access. For this purpose, \cheetah{} should quickly locate its corresponding cache line. \Cheetah{} utilizes the shadow memory mechanism to speed up this locating procedure~\cite{qinzhao, Predator}. 
%Shadow memory has been utilized extensively in different fields, such as detecting concurrency bugs~\cite{Harrow:2000:RCM:645880.672080, helgrind, 404681, Savage:1997:EDD:268998.266641}, tracking information flow or data flow~\cite{Cheng:2006:TEF:1157733.1157903, Newsome05dynamictaint, Qin:2006:LLP:1194816.1194834}, or detecting memory errors or others~\cite{qinzhao, Hastings91purify:fast, Seward:2005:UVD:1247360.1247362, Narayanasamy:2006:ALO:1140277.1140303}.  
% FIXME, if we need less citations. 
To utilize the shadow memory mechanism, we should determine the range of heap memory, which is difficult to know beforehand when using the default heap. Thus, \cheetah{} builds its custom heap based on Heap Layers~\cite{Berger:2001:CHM:378795.378821}. \cheetah{} pre-allocates a fixed-size memory block (using \texttt{mmap}) and satisfies all memory allocations from this block. \cheetah{} adapts the per-thread heap organization used by Hoard so that two objects in the same cache line will never be allocated to two different threads~\cite{Hoard}. This design prevents inter-objects false sharing, but also makes \cheetah{} unable to report problems that are possibly caused by the default heap allocator.  
%However, we argue that this problem should be fixed by using a modern heap allocator like Hoard~\cite{Hoard}. 

\paragraph{Implementation} 
To use its custom heap, \cheetah{} intercepts all memory allocations and deallocations. \cheetah{} initializes the heap before an application enters the \texttt{main} routine. \cheetah{} maintains two different heaps, one for the application and one for internal uses. For both heaps, \cheetah{} manages objects based on the unit of {\it power of two}. For each memory allocation from the application, \cheetah{} saves the information of callsite and size, which helps \cheetah{} to precisely report the line information of falsely-shared objects.  
%heap and globals. 
%Two huge arrays. 
%\cheetah{} keeps track of memory accesses of global variables and objects of the application heap using the shadow memory technique.
\Cheetah{} allocates two large arrays (using \texttt{mmap}) to track the number of writes and detailed access information on each cache line. For each memory access, \cheetah{} uses bit-shifting to compute the index of its cache line. 


\subsection{Computing Cache Invalidations}
\label{sec:computeinvalidations}

%Section~\ref{sec:detect} discusses a general rule to compute the number of cache invalidations. 
Prior work of Zhao et al. proposes an ownership-based method to compute the number of cache invalidations: when a thread updates a cache line owned by others, this access incurs an cache invalidation and then resets the ownership to the current thread~\cite{qinzhao}. However, this approach cannot easily scale to more than 32 threads because of excessive memory consumption, since it needs one bit for every thread to track the ownership.  

%To track the number of cache invalidations,  \Cheetah{} keeps a counter for every cache line.  
To address this problem, \Cheetah{} maintains a two-entry table ($T$) for each cache line ($L$), where each thread will at most occupy one of these two entries. In this table, each entry has two fields, thread ID and access type (read or write). It computes the number of invalidations according to the rule described in Section~\ref{sec:detect}. In case of a cache invalidation, the current access (write) is added into the corresponding table. Thus, each table always has at least one entry. More specifically, \cheetah{} handles each access as follows.

%a write access from a thread different with existing entries can cause a cache invalidation. When there is a cache invalidation, 
\begin{itemize}
\item
For each read access, \cheetah{} decides whether to record this entry. If the table $T$ is not full and the existing entry is coming from a different thread (with a different ID), \cheetah{} records this read access in the table. Otherwise, there is no need to handle this read access. 
  
\item
For each write access, \cheetah{} decides whether this access incurs an invalidation. If the table is already full, based on assumption 1, it incurs a cache invalidation since at least one of existing entries (of this table) is from a different thread. If the table is not full (and not empty), \cheetah{} checks whether the existing entry is from a different thread or not. If this write access is from the same thread as the existing entry, \cheetah{} skips the current write access since there is no need to update the existing entry. In all other cases, this write access incurs at least a cache invalidation. Currently, \cheetah{} does not differentiate how many reads have happened before this write. In case of a cache invalidation, the table is flushed and the write access are recorded in the table to maintain the table not empty. 
  
\end{itemize}
     
%When there is an memory access, \Cheetah{} checks against its two-entries-cache-history table for the current cache line and determines whether an access leads to a cache invalidation according to the rule discussed above. 

\paragraph{Implementation} 
As aforementioned, only cache lines with a large number of writes can possibly have a high impact on the performance. Based on this observation, cache lines with a small number of writes are never the causes of the severe performance degradation. For this reason, \Cheetah{} tracks the number of writes on a cache line at first, and only tracks detailed information for cache lines with more than two writes. This simple policy avoids tracking detailed information for write-once memory. 

 \subsection{Reporting False Sharing}
% How we will report false sharing precisely and correctly?
% How we 
\label{sec:report}

\Cheetah{} reports false sharing correctly and precisely, either at the end of an execution or an interruption by the user.  

\paragraph{Correct Detection.} \Cheetah{} tracks word-based (four bytes) memory accesses on susceptible cache lines using the shadow memory technique: how many reads or writes are issued by a particular thread on each word. When more than one thread access a word, \Cheetah{} marks this word to be shared by multiple threads. By identifying accesses on each word of a susceptible cache line, we can easily differentiate false sharing from true sharing since multiple threads will access the same words in true sharing. Word-based information also helps programmers to decide how to pad a problematic data structure in fixing phases. It is very common that the main thread may allocate and initialize objects before they are accessed by multiple children threads. Prior work, including Predator~\cite{Predator}, may wrongly report them as true sharing instances. \cheetah{} avoids this problem by only recording detailed accesses inside parallel phases.

\paragraph{Precise Detection.} \Cheetah{} reports precise information for global variables and heap objects that are involved in false sharing. For global variables, \Cheetah{} reports names and addresses by searching through the symbol table in the binary executable. For heap objects, \Cheetah{} reports the lines of code for their allocation sites. Thus, \Cheetah{} intercepts all memory allocations and de-allocations to obtain the whole call stack. 
\cheetah{} does not monitor stack variables because they are normally accessed only by their hosting threads. It is noted that the default \texttt{backtrace} function in \texttt{glibc} is extremely slow due to expensive instruction analysis. \cheetah{} utilizes the frame pointers to fetch the call stack efficiently. Moreover, we only collect five function entries on the call stack for performance reasons.
%\todo{Check the last sentence I added is correct or not.} 
%During the real implementation, we tried to keep the overhead of getting the callsite stack as little as possible. \cheetah{} utilizes a global hash table to save those known callsite stack. The combination of ``rip'' (instruction pointer) and ``stack offset'' is considered as the key of this global hash table. If the combination of these two values (as the key) have existed in the global hash table, we simply copied the saved callsite stack to a new object. Otherwise, backtrace() is called to fetch the callstack.  

 
%\cheetah{} only reports those false sharing problems that can have a significant impact on the performance by predicting the upper bound of performance improvement, according to the idea that are discussed in Section~\ref{sec:predictidea}.  It will rank the severity of performance degradation of any detected false sharing problems based on the predicted performance improvement after fixes.