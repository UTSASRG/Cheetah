\section{Detecting False Sharing}
\label{sec:detect}

% What is the basic idea? Why those ones uses performance counters can not reveal false sharing problems? 
\cheetah{} aims to report false sharing with significant performance impact. Since only cache lines with a large number of cache invalidations can have big performance impact on the performance, \cheetah{} should locate those cache lines. However, this task turns out to be difficult because cache invalidation depends on the pattern of memory accesses, actual cache hierarchy and thread-to-core mappings. 

Thus, \cheetah{} proposes a simple rule to track cache invalidations: {\emph if a thread writes a cache line after other threads have accessed the same cache line, this write operation causes a cache invalidation}. This rule is based on the following two assumptions:
 
%Though hardware performance counters can collect events related to the cache invalidation, they have difficulties in identifying its root causes, such as instructions and data objects involved in the cache invalidation. In contrast, directly analyzing the memory traces collected by PMU can give more insights. Given a set of memory addresses, one typically needs the configuration of cache hierarchy and the knowledge of thread-to-core mappings to perform accurate analysis. 

\begin{itemize} 
\item {\bf Assumption 1:} Each thread runs on a separate core with its own private cache. 

\item {\bf Assumption 2: } Cache sizes are infinite. 
 
\end{itemize}

Assumption 1 is reasonable because thread over-subscription is generally rare for computation intensive programs. This assumption just over-reports the number of cache invalidations, if multiple threads are actually scheduled to the same core or different cores may share part of cache hierarchy. In this meaning, assumption 1 actually presents the worst scenario for a given false sharing instance. With this assumption, \cheetah{} is independent from thread-to-core mapping or actual cache hierarchy.

Assumption 2 further defines the behavior related to cache eviction and invalidation. If there is a memory access within a cache line, the hardware cache (of running this thread) always holds the data until an access of other threads (running on other cores by assumption 1) invalidates it. This assumption avoids to track cache evictions caused by cache capacity. 
%However, 
%If there is a memory access within a cache line, the hardware cache (of running this thread) always holds the data until an access of other threads (running on other cores by assumption 1) invalidates it. Thus, there is no need to track cache evictions that are caused by cache capacity. 

By combing with these two assumptions, \cheetah{} can identify cache validations simply based on the pattern of memory accesses, without the exact knowledge of actual memory hierarchy and execution condition. This idea is similar to the prior work~\cite{Predator, qinzhao}. 

In the remaining of this section, we elaborate how we install PMU-based sampling mechanism for memory accesses in Section~\ref{sec:perfcounter}, how we track memory accesses and data allocation in Section~\ref{sec:shadow}, how we analyze access patterns for cache invalidation in Section~\ref{sec:computeinvalidations}, and how we report false sharing in Section~\ref{sec:report}.

\subsection{Sampling Memory Accesses}
\label{sec:perfcounter}

According to the basic rule described above, it is important to track memory accesses of different threads in order to compute the number of cache invalidations on each cache line. Software based approaches may introduce higher than $5\times$ performance overhead~\cite{Predator, qinzhao}. This high overhead can block people to use these tools in real deployment.

%The state-of-the-art work \Predator{} leverages the compiler instrumentation to insert function calls before every memory access~\cite{Predator}. However, this approach introduces more than $5\times$ performance overhead by instrumenting every memory access. Moreover, \Predator{} has to re-compile applications, which needs the availability of source code. \Predator{} is not desirable for legacy applications without the source code, or real deployment that is sensitive to performance. 
\cheetah{} aims to significantly reduce the performance overhead by leveraging PMU-based sampling (i.e., AMD IBS and Intel PEBS) that are available in most modern architectures. 
For each sample, PMU distinguishes whether it is a memory read or write, captures the memory address touched by the sampled memory access, and records the thread ID that triggers this sample. Those information will be utilized to analyze whether a sample leads to a cache invalidation or not, based on the basic rule described in Section~\ref{sec:detect}.

%These information is enough to compute the number of cache invalidations based on the basic rule that is described in Section~\ref{sec:basicidea}. 
Since hardware performance counter only samples one memory access out of a specified number, it doesn't pose significant performance overhead. Besides that, using performance counters does not need to instrument source code explicitly, thus providing a non-intrusive way to monitor memory references. 

\paragraph{Implementation.} 

In order to sample memory accesses, \cheetah{} programs the PMU registers to turn on sampling with a pre-defined threshold, before entering the \texttt{main} routine of the application. It also installs a signal handler to associate with the sampling so that we can collect detailed memory accesses. In order to simplify the signal handling, \Cheetah{} configures the signal handler to be responded by the current thread, by calling \texttt{fcntl} function with \texttt{F\_SETOWN\_EX} flag. \cheetah{} also sets up the custom allocator and its internal heap in the initialization.

Inside the signal handler, \Cheetah{} collects detailed information of every sampled memory access, including its memory address, thread id, read or write operation, and access latency, which can be fed into the computing module to compute the number of cache invalidations and the prediction module to predict performance impact.

\subsection{Locating Cache Line}
\label{sec:shadow}

For each sampled memory access, \cheetah{} will decide whether this access causes a cache invalidation or not and records the detailed information about this access. For this purpose, \cheetah{} should quickly locate an access's related cache line. \Cheetah{} utilizes the shadow memory mechanism to speedup this locating procedure.
Shadow memory has been utilized extensively in different fields, such as detecting concurrency bugs~\cite{Harrow:2000:RCM:645880.672080, helgrind, 404681, Savage:1997:EDD:268998.266641}, tracking information flow or data flow~\cite{Cheng:2006:TEF:1157733.1157903, Newsome05dynamictaint, Qin:2006:LLP:1194816.1194834}, or detecting memory errors or others~\cite{qinzhao, Hastings91purify:fast, Seward:2005:UVD:1247360.1247362, Narayanasamy:2006:ALO:1140277.1140303}.  
% FIXME, if we need less citations. 

To utilize the shadow memory mechanism, we should determine the range of heap memory, which is difficult to know beforehand if using the default heap. Thus, \cheetah{} built its custom heap based on Heap Layers~\cite{Berger:2001:CHM:378795.378821}. \cheetah{} pre-allocates a fixed size of memory
from its underlying operating system using \texttt{mmap} system calls and satisfies memory allocations from this block of memory. \cheetah{} also adapts the per-thread heap organization used by Hoard~\cite{Hoard} so that two objects in the same cache line will never be allocated to two different threads. This design prevents inter-objects false sharing, but also makes \cheetah{} can not report false sharing problems caused by the heap allocator.  However, we argue that this problem should be fixed by using a modern heap allocator like Hoard~\cite{Hoard}. 

\paragraph{Implementation} 
To use its custom heap, \cheetah{} intercepts all memory allocations and deallocations. \cheetah{} initializes the heap before the application enters into \texttt{main} routine, by putting the initialization routine into the constructor attribute. \cheetah{} maintains two different heaps, where all memory usage of the application will be satisfied from its application heap and other memory usage will be allocated from its internal heap. For both heaps, \cheetah{} manages objects based on the unit of {\it power of two}. For each memory allocation from the application, \cheetah{} saves the information of callsite and size by adding them to an object header, just before each object. This helps \cheetah{} to precisely report the line information of falsely-shared objects.  

%heap and globals. 
%Two huge arrays. 
%\cheetah{} keeps track of memory accesses of global variables and objects of the application heap using the shadow memory technique.
\Cheetah{} allocates two large arrays (by using \texttt{mmap}) to keep track of the number of writes and detailed access information on each cache line. For each memory address, \cheetah{} uses the bit shift to compute the index of cache line and locates the placement of its corresponding array quickly. 


\subsection{Computing Cache Invalidations}
\label{sec:computeinvalidations}

Section~\cite{sec:detect} discusses a general rule to compute the number of cache invalidations. Prior work of Zhao et. al. proposed a method based on the ownership of cache lines: when a thread updates an object that it does not own, it will cause cache invalidations and reset the owner to the current thread~\cite{qinzhao}. However, their approach cannot be easily to scale to more than 32 threads because of excessive memory consumption, where every word with 32-bits can only track 32 threads.  

To overcome this problem, \Cheetah{} keeps a two-entries-table ($T$) for each cache line ($L$), which is borrowed from prior work Predator~\cite{Predator}. Each entry of this table has two fields,   thread ID and access type (read or write). \Cheetah{} also keeps a counter for every cache line that indicates the number of cache invalidations on this cache line.  

According to the basic rule that are described in Section~\ref{sec:basicidea}, only a write access that is from a different thread with existing entries can cause a cache invalidation. When there is a cache invalidation, the current access (write) will flush the table and will be added into its corresponding table ($T$). Thus, a table will always have an entry except in the beginning. More specifically, \cheetah{} checks possible cache invalidations as follows.
 
\begin{itemize}
\item
  For each read access, \cheetah{} will decide whether to record this entry. If the table $T$ is not full and the existing entry is coming from a different thread (with a different ID), \cheetah{} will record this read access in the table.
  
  \item
  For each write access, \cheetah{} will decide whether there is an invalidation. If the table is already full, this write access will lead to a cache invalidation because at least one existing entry is issued by a different thread (running on a different core, assumption 1). If the table is not full (and not empty), this write access will lead to a cache invalidation only if the existing entry is from a different thread. \cheetah{} does nothing if this write is the same as the existing entry. 
  
\end{itemize}
     
%When there is an memory access, \Cheetah{} checks against its two-entries-cache-history table for the current cache line and determines whether an access leads to a cache invalidation according to the rule discussed above. 

\paragraph{Implementation.} 
As discussed before, only cache lines with a big number of writes can possibly have a big impact on the performance. Based on this observation, cache lines with a small number of writes are never be a target that can cause serious performance problem. For this reason, \Cheetah{} tracks the number of writes on a cache line at first, and only tracks detailed information when this number is larger than a pre-defined threshold. Using this method can also save memory since \cheetah{} only allocates memory to record word-level read/write access: what is the address of an access; whether this is a read or write access; which thread issues this access. All of these are going to be checked in the reporting phase that is described in Section~\ref{sec:report}. 

 \subsection{Reporting False Sharing}
% How we will report false sharing precisely and correctly?
% How we 
\label{sec:report}

\Cheetah{} reports false sharing correctly and precisely. \Cheetah{} invokes the process of checking and reporting false sharing problems, either at the end of programs or receiving the instructions from users through the \texttt{SIGUSR2} signal.  

\paragraph{Correct Detection.} \Cheetah{} keeps track of word-based (four bytes) memory accesses on susceptible cache lines: how many reads or writes occurs by which thread on each word. When more than one thread access a word, \Cheetah{} marks this word to be shared. By identifying accesses on each word on a susceptible cache line, we can easily differentiate false sharing from true sharing. Word-based information can also help diagnose false sharing problems in more detail, which helps programmers to decide how to pad an existing data structure in order to avoid false sharing. Because it is possible for a thread, particularly the main thread, to allocate an object and do some initialization before passing to different threads, \cheetah{} only tracks the information of memory accesses inside parallel phases to avoid this problem.

\paragraph{Precise Detection.} \Cheetah{} reports precise information for global variables and heap objects that are involved in false sharing. For global variables, \Cheetah{} reports names and addresses by searching through the ELF symbol table. For heap objects, \Cheetah{} reports the lines of code for allocating these objects. Thus, \Cheetah{} intercepts all memory allocations and de-allocations to obtain the whole callsite stack. 
%During the real implementation, we tried to keep the overhead of getting the callsite stack as little as possible. \cheetah{} utilizes a global hash table to save those known callsite stack. The combination of ``rip'' (instruction pointer) and ``stack offset'' is considered as the key of this global hash table. If the combination of these two values (as the key) have existed in the global hash table, we simply copied the saved callsite stack to a new object. Otherwise, backtrace() is called to fetch the callstack.  

 
%\cheetah{} only reports those false sharing problems that can have a significant impact on the performance by predicting the upper bound of performance improvement, according to the idea that are discussed in Section~\ref{sec:predictidea}.  It will rank the severity of performance degradation of any detected false sharing problems based on the predicted performance improvement after fixes.